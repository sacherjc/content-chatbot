{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sacherjc/content-chatbot/blob/main/Shareable_copy_of_CapsidChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your own chatbot!\n",
        "Based on this tutorial from Mark Paepper: https://www.paepper.com/blog/posts/build-q-and-a-bot-of-your-website-using-langchain/"
      ],
      "metadata": {
        "id": "QN70vpxI7rc_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTay2_KAzfh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc403dd-b3d7-49ab-ebb4-d2fefbb7d3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'content-chatbot'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 36 (delta 14), reused 31 (delta 12), pack-reused 1\u001b[K\n",
            "Unpacking objects: 100% (36/36), 272.76 KiB | 6.99 MiB/s, done.\n",
            "/content/content-chatbot\n"
          ]
        }
      ],
      "source": [
        "# First, make a copy of this notebook! That way you'll be able to save it\n",
        "\n",
        "# this clones the github from Mark P's tutorial\n",
        "!git clone https://github.com/mpaepper/content-chatbot.git\n",
        "\n",
        "# This changes the directory to content-chatbot\n",
        "%cd content-chatbot/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this installs the requirements according to what was in Mark P's tutorial\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "GCDA2oQV0HtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This lets you add your API key so you can access OpenAI's large language models (e.g. ChatGPT), which the code below will call on\n",
        "import os\n",
        "\n",
        "# add your OpenAI API key here, between the quotes\n",
        "os.environ['OPENAI_API_KEY'] = ''\n"
      ],
      "metadata": {
        "id": "-34NUkSL011Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Run this so you can access your Google drive files from Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pzuRXeo0vfwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a1de364-5350-48d3-f760-824c4240dd35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before running this code, create a new file in the content-chatbot folder called\n",
        "# links.txt (get it here for Capsid & Tail links: https://drive.google.com/file/d/1YB5LuhNsr_v-WjloYiPoxZSsIUel_TW0/view?usp=share_link)\n",
        "# *note, Google Colab doesn't save files after a session is closed\n",
        "# if you want to save for later, make sure there's a copy of it in your Google Drive too)\n",
        "# Open the links.txt file here in Colab by double clicking on it (left panel under Files)\n",
        "# Paste in your URLs into the empty links.txt file (I used Capsid & Tail blog post URLS,\n",
        "# but you can use any URLs you're interested in!) and hit command+S to save the file. One link on each line.\n",
        "\n",
        "\n",
        "# this opens/reads your links.txt file\n",
        "with open('links.txt', 'r') as f:\n",
        "  links = f.read()\n",
        "\n",
        "# this creates a list of links\n",
        "links_split = links.split('\\n')\n",
        "\n",
        "# this prints your list below so you can make sure it worked\n",
        "print(links_split)"
      ],
      "metadata": {
        "id": "9WFKMi23q9TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block scrapes the text from each of the links in your links_split list\n",
        "\n",
        "# First, it imports a bunch of stuff it needs\n",
        "import argparse\n",
        "import pickle\n",
        "import requests\n",
        "import xmltodict\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# this function extracts text from a URL\n",
        "def extract_text_from(url):\n",
        "    html = requests.get(url).text\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    return '\\n'.join(line for line in lines if line)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='Embedding website content')\n",
        "    parser.add_argument('-s', '--sitemap', type=str, required=False,\n",
        "            help='URL to your sitemap.xml', default='https://www.paepper.com/sitemap.xml')\n",
        "    parser.add_argument('-f', '--filter', type=str, required=False,\n",
        "            help='Text which needs to be included in all URLs which should be considered',\n",
        "            default='https://www.paepper.com/blog/posts')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    r = requests.get(args.sitemap)\n",
        "    xml = r.text\n",
        "    raw = xmltodict.parse(xml)\n",
        "\n",
        "# This will loop through each URL in the links_split list, extract the text from each webpage, and save the text + the URL in a dictionary.\n",
        "\n",
        "    # this creates a new empty dictionary called pages\n",
        "    pages = []\n",
        "\n",
        "    # this is a 'for loop' which goes through each url in the links_split list one by one. For each one, adds it to the pages dictionary\n",
        "    for url in links_split:\n",
        "            pages.append({'text': extract_text_from(url), 'source': url})\n",
        "\n",
        "# This is the splitter - splits your loaded text into chunks and stores the chunks\n",
        "    text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
        "    docs, metadatas = [], []\n",
        "    for page in pages:\n",
        "        splits = text_splitter.split_text(page['text'])\n",
        "        docs.extend(splits)\n",
        "        metadatas.extend([{\"source\": page['source']}] * len(splits))\n",
        "        print(f\"Split {page['source']} into {len(splits)} chunks\")\n",
        "\n",
        "    store = FAISS.from_texts(docs, OpenAIEmbeddings(), metadatas=metadatas)\n",
        "    with open(\"faiss_store.pkl\", \"wb\") as f:\n",
        "        pickle.dump(store, f)"
      ],
      "metadata": {
        "id": "b2OL9iXZ9_H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask questions here! Remove hashtags to 'run' the questions (# stops code from running)\n",
        "\n",
        "#!python ask_question.py \"Are people using qpcr for phage therapy?\"\n",
        "\n",
        "#!python ask_question.py \"What is the State of Phage survey, how many articles feature it, and what did it find?\"\n",
        "\n",
        "# !python ask_question.py \"What is the STAMP trial?\"\n",
        "\n",
        "!python ask_question.py \"What is a spreadsheet useful for in the phage space?\"\n",
        "\n",
        "!python ask_question.py \"Are phages useful in urinary infections? What has been their track record?\"\n",
        "\n",
        "#!python ask_question.py \"Who treated a sea turtle with phages?\"\n",
        "#!python ask_question.py \"List the australian phage researchers we have talked about in capsid & tail\"\n",
        "#!python ask_question.py \"How might you count phages during phage therapy?\"\n"
      ],
      "metadata": {
        "id": "CECmXCDxEXU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be294ba-5e61-4a82-d5ac-99a5b95b1e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new VectorDBQAWithSourcesChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer:  Spreadsheets are useful for laying out two-dimensional data and tracking relationships in a manual way.\n",
            "\n",
            "Sources: https://phage.directory/capsid/how-to-organize-phage-biobank-data, https://phage.directory/capsid/tabular-data\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new VectorDBQAWithSourcesChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Answer:  Phages have been used to reduce biofilm biomass in a human urine model and have been administered to patients in a phase 1/2 clinical trial to evaluate their PhageBank™ technology as a treatment for UTIs.\n",
            "\n",
            "Sources: \n",
            "https://phage.directory/capsid/rumen-phage\n",
            "https://phage.directory/capsid/salvage-phage-therapy\n",
            "https://phage.directory/capsid/phage-therapy-access-india\n",
            "https://phage.directory/capsid/go-viral-adaptive-phage-therapeutics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to make a Chat App version of the above\n",
        "import pickle\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ChatVectorDBChain\n",
        "\n",
        "_template = \"\"\"Given the following conversation and a follow up question,\n",
        "rephrase the follow up question to be a standalone question.\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
        "\n",
        "template = \"\"\"You are an AI assistant for answering questions about\n",
        "phage-related blog posts published on Capsid & Tail.\n",
        "You are given the following extracted parts of\n",
        "a long document and a question. Provide a conversational answer.\n",
        "If you don't know the answer, just say \"Hmm, I'm not sure.\".\n",
        "Don't try to make up an answer. If the question is not about\n",
        "phages or phage therapy, politely inform them that you are tuned\n",
        "to only answer questions about phages.\n",
        "Question: {question}\n",
        "=========\n",
        "{context}\n",
        "=========\n",
        "Answer in Markdown:\"\"\"\n",
        "QA = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
        "\n",
        "\n",
        "def get_chain(vectorstore):\n",
        "    llm = OpenAI(temperature=0)\n",
        "    qa_chain = ChatVectorDBChain.from_llm(\n",
        "        llm,\n",
        "        vectorstore,\n",
        "        qa_prompt=QA,\n",
        "        condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"faiss_store.pkl\", \"rb\") as f:\n",
        "        vectorstore = pickle.load(f)\n",
        "    qa_chain = get_chain(vectorstore)\n",
        "    chat_history = []\n",
        "    print(\"Chat with the Capsid & Tail bot:\")\n",
        "    while True:\n",
        "        print(\"Your question:\")\n",
        "        question = input()\n",
        "        result = qa_chain({\"question\": question, \"chat_history\": chat_history})\n",
        "        chat_history.append((question, result[\"answer\"]))\n",
        "        print(f\"AI: {result['answer']}\")"
      ],
      "metadata": {
        "id": "YJ9MxRSMJPWk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}